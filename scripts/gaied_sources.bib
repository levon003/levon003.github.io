
@inproceedings{li_beyond_2023,
	address = {New York, NY, USA},
	series = {{UIST} '23},
	title = {Beyond the {Artifact}: {Power} as a {Lens} for {Creativity} {Support} {Tools}},
	isbn = {9798400701320},
	shorttitle = {Beyond the {Artifact}},
	url = {https://dl.acm.org/doi/10.1145/3586183.3606831},
	doi = {10.1145/3586183.3606831},
	abstract = {Researchers who build creativity support tools (CSTs) define abstractions and software representations that align with user needs to give users the power to accomplish tasks. However, these specifications also structure and limit how users can and should think, act, and express themselves. Thus, tool designers unavoidably exert power over their users by enacting a “normative ground” through their tools. Drawing on interviews with 11 creative practitioners, tool designers, and CST researchers, we offer a definition of empowerment in the context of creative practice, build a preliminary theory of how power relationships manifest in CSTs, and explain why researchers have had trouble addressing these concepts in the past. We re-examine CST literature through a lens of power and argue that mitigating power imbalances at the level of technical design requires enabling users in both vertical movement along levels of abstraction as well as horizontal movement between tools through interoperable representations. A lens of power is one possible orientation that lets us recognize the methodological shifts required towards building “artistic support tools.”},
	urldate = {2023-12-16},
	booktitle = {Proceedings of the 36th {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Li, Jingyi and Rawn, Eric and Ritchie, Jacob and Tran O'Leary, Jasper and Follmer, Sean},
	month = oct,
	year = {2023},
	pages = {1--15},
	file = {Full Text PDF:/Users/zacharylevonian/Zotero/storage/638DR3GJ/Li et al. - 2023 - Beyond the Artifact Power as a Lens for Creativit.pdf:application/pdf},
}

@book{mark_attention_2023,
	edition = {Original edition},
	title = {Attention {Span}: {A} {Groundbreaking} {Way} to {Restore} {Balance}, {Happiness} and {Productivity}},
	isbn = {978-1-335-44941-2},
	shorttitle = {Attention {Span}},
	language = {English},
	publisher = {Hanover Square Press},
	author = {Mark, Gloria},
	month = jan,
	year = {2023},
}

@misc{sherpa_labs_sherpa_nodate,
	title = {Sherpa},
	url = {https://sherpalabs.co/},
	abstract = {Conversation breeds understanding},
	language = {en},
	urldate = {2023-12-16},
	author = {{Sherpa Labs}},
	file = {Snapshot:/Users/zacharylevonian/Zotero/storage/24EFFTQ6/sherpalabs.co.html:text/html},
}

@misc{nie_play_2021,
	title = {Play to {Grade}: {Testing} {Coding} {Games} as {Classifying} {Markov} {Decision} {Process}},
	shorttitle = {Play to {Grade}},
	url = {http://arxiv.org/abs/2110.14615},
	doi = {10.48550/arXiv.2110.14615},
	abstract = {Contemporary coding education often presents students with the task of developing programs that have user interaction and complex dynamic systems, such as mouse based games. While pedagogically compelling, there are no contemporary autonomous methods for providing feedback. Notably, interactive programs are impossible to grade by traditional unit tests. In this paper we formalize the challenge of providing feedback to interactive programs as a task of classifying Markov Decision Processes (MDPs). Each student's program fully specifies an MDP where the agent needs to operate and decide, under reasonable generalization, if the dynamics and reward model of the input MDP should be categorized as correct or broken. We demonstrate that by designing a cooperative objective between an agent and an autoregressive model, we can use the agent to sample differential trajectories from the input MDP that allows a classifier to determine membership: Play to Grade. Our method enables an automatic feedback system for interactive code assignments. We release a dataset of 711,274 anonymized student submissions to a single assignment with hand-coded bug labels to support future research.},
	urldate = {2023-12-16},
	publisher = {arXiv},
	author = {Nie, Allen and Brunskill, Emma and Piech, Chris},
	month = dec,
	year = {2021},
	note = {arXiv:2110.14615 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Machine Learning},
	annote = {https://anie.me/play2grade/
arXiv Comment: NeurIPS 2021, 16 pages, 7 figures
},
	file = {arXiv Fulltext PDF:/Users/zacharylevonian/Zotero/storage/P2EQIFZG/Nie et al. - 2021 - Play to Grade Testing Coding Games as Classifying.pdf:application/pdf;arXiv.org Snapshot:/Users/zacharylevonian/Zotero/storage/LUQN2RQM/2110.html:text/html},
}

@inproceedings{chaurasia_dialog_2017,
	address = {Taipei, Taiwan},
	title = {Dialog for {Language} to {Code}},
	url = {https://aclanthology.org/I17-2030},
	abstract = {Generating computer code from natural language descriptions has been a long-standing problem. Prior work in this domain has restricted itself to generating code in one shot from a single description. To overcome this limitation, we propose a system that can engage users in a dialog to clarify their intent until it has all the information to produce correct code. To evaluate the efficacy of dialog in code generation, we focus on synthesizing conditional statements in the form of IFTTT recipes.},
	urldate = {2023-12-16},
	booktitle = {Proceedings of the {Eighth} {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 2: {Short} {Papers})},
	publisher = {Asian Federation of Natural Language Processing},
	author = {Chaurasia, Shobhit and Mooney, Raymond J.},
	editor = {Kondrak, Greg and Watanabe, Taro},
	month = nov,
	year = {2017},
	pages = {175--180},
	file = {Full Text PDF:/Users/zacharylevonian/Zotero/storage/6E6KFYI7/Chaurasia and Mooney - 2017 - Dialog for Language to Code.pdf:application/pdf},
}

@article{halevy_unreasonable_2009,
	title = {The {Unreasonable} {Effectiveness} of {Data}},
	volume = {24},
	issn = {1541-1672},
	url = {http://ieeexplore.ieee.org/document/4804817/},
	doi = {10.1109/MIS.2009.36},
	language = {en},
	number = {2},
	urldate = {2023-12-16},
	journal = {IEEE Intelligent Systems},
	author = {Halevy, Alon and Norvig, Peter and Pereira, Fernando},
	month = mar,
	year = {2009},
	pages = {8--12},
	file = {Halevy et al. - 2009 - The Unreasonable Effectiveness of Data.pdf:/Users/zacharylevonian/Zotero/storage/4WS5HW9K/Halevy et al. - 2009 - The Unreasonable Effectiveness of Data.pdf:application/pdf},
}

@misc{zhang_merging_2023,
	title = {Merging {Generated} and {Retrieved} {Knowledge} for {Open}-{Domain} {QA}},
	url = {http://arxiv.org/abs/2310.14393},
	doi = {10.48550/arXiv.2310.14393},
	abstract = {Open-domain question answering (QA) systems are often built with retrieval modules. However, retrieving passages from a given source is known to suffer from insufficient knowledge coverage. Alternatively, prompting large language models (LLMs) to generate contextual passages based on their parametric knowledge has been shown to improve QA performance. Yet, LLMs tend to "hallucinate" content that conflicts with the retrieved knowledge. Based on the intuition that answers supported by both sources are more likely to be correct, we propose COMBO, a Compatibility-Oriented knowledge Merging for Better Open-domain QA framework, to effectively leverage the two sources of information. Concretely, we match LLM-generated passages with retrieved counterparts into compatible pairs, based on discriminators trained with silver compatibility labels. Then a Fusion-in-Decoder-based reader model handles passage pairs to arrive at the final answer. Experiments show that COMBO outperforms competitive baselines on three out of four tested open-domain QA benchmarks. Further analysis reveals that our proposed framework demonstrates greater efficacy in scenarios with a higher degree of knowledge conflicts.},
	urldate = {2023-12-16},
	publisher = {arXiv},
	author = {Zhang, Yunxiang and Khalifa, Muhammad and Logeswaran, Lajanugen and Lee, Moontae and Lee, Honglak and Wang, Lu},
	month = oct,
	year = {2023},
	note = {arXiv:2310.14393 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: EMNLP 2023 - Camera Ready},
	file = {arXiv Fulltext PDF:/Users/zacharylevonian/Zotero/storage/TPTAYIN6/Zhang et al. - 2023 - Merging Generated and Retrieved Knowledge for Open.pdf:application/pdf;arXiv.org Snapshot:/Users/zacharylevonian/Zotero/storage/EJBVJXQF/2310.html:text/html},
}

@inproceedings{lu_readingquizmaker_2023,
	address = {Hamburg Germany},
	title = {{ReadingQuizMaker}: {A} {Human}-{NLP} {Collaborative} {System} that {Supports} {Instructors} to {Design} {High}-{Quality} {Reading} {Quiz} {Questions}},
	isbn = {978-1-4503-9421-5},
	shorttitle = {{ReadingQuizMaker}},
	url = {https://dl.acm.org/doi/10.1145/3544548.3580957},
	doi = {10.1145/3544548.3580957},
	language = {en},
	urldate = {2023-12-16},
	booktitle = {Proceedings of the 2023 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Lu, Xinyi and Fan, Simin and Houghton, Jessica and Wang, Lu and Wang, Xu},
	month = apr,
	year = {2023},
	pages = {1--18},
	file = {Lu et al. - 2023 - ReadingQuizMaker A Human-NLP Collaborative System.pdf:/Users/zacharylevonian/Zotero/storage/SMDRUSNK/Lu et al. - 2023 - ReadingQuizMaker A Human-NLP Collaborative System.pdf:application/pdf},
}

@misc{schmucker_ruffleriley_2023,
	title = {Ruffle\&{Riley}: {Towards} the {Automated} {Induction} of {Conversational} {Tutoring} {Systems}},
	shorttitle = {Ruffle\&{Riley}},
	url = {http://arxiv.org/abs/2310.01420},
	doi = {10.48550/arXiv.2310.01420},
	abstract = {Conversational tutoring systems (CTSs) offer learning experiences driven by natural language interaction. They are known to promote high levels of cognitive engagement and benefit learning outcomes, particularly in reasoning tasks. Nonetheless, the time and cost required to author CTS content is a major obstacle to widespread adoption. In this paper, we introduce a novel type of CTS that leverages the recent advances in large language models (LLMs) in two ways: First, the system induces a tutoring script automatically from a lesson text. Second, the system automates the script orchestration via two LLM-based agents (Ruffle\&Riley) with the roles of a student and a professor in a learning-by-teaching format. The system allows a free-form conversation that follows the ITS-typical inner and outer loop structure. In an initial between-subject online user study (N = 100) comparing Ruffle\&Riley to simpler QA chatbots and reading activity, we found no significant differences in post-test scores. Nonetheless, in the learning experience survey, Ruffle\&Riley users expressed higher ratings of understanding and remembering and further perceived the offered support as more helpful and the conversation as coherent. Our study provides insights for a new generation of scalable CTS technologies.},
	urldate = {2023-12-16},
	publisher = {arXiv},
	author = {Schmucker, Robin and Xia, Meng and Azaria, Amos and Mitchell, Tom},
	month = nov,
	year = {2023},
	note = {arXiv:2310.01420 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Human-Computer Interaction},
	annote = {Comment: NeurIPS'23 GAIED, Camera-ready},
	file = {arXiv Fulltext PDF:/Users/zacharylevonian/Zotero/storage/CI3DY2FT/Schmucker et al. - 2023 - Ruffle&Riley Towards the Automated Induction of C.pdf:application/pdf;arXiv.org Snapshot:/Users/zacharylevonian/Zotero/storage/P6DXDXT5/2310.html:text/html},
}

@inproceedings{kim_student_2023,
	address = {Arlington TX USA},
	title = {The {Student} {Zipf} {Theory}: {Inferring} {Latent} {Structures} in {Open}-{Ended} {Student} {Work} {To} {Help} {Educators}},
	isbn = {978-1-4503-9865-7},
	shorttitle = {The {Student} {Zipf} {Theory}},
	url = {https://dl.acm.org/doi/10.1145/3576050.3576116},
	doi = {10.1145/3576050.3576116},
	language = {en},
	urldate = {2023-12-16},
	booktitle = {{LAK23}: 13th {International} {Learning} {Analytics} and {Knowledge} {Conference}},
	publisher = {ACM},
	author = {Kim, Yunsung and Piech, Chris},
	month = mar,
	year = {2023},
	pages = {464--475},
	file = {Kim and Piech - 2023 - The Student Zipf Theory Inferring Latent Structur.pdf:/Users/zacharylevonian/Zotero/storage/UA7S55I7/Kim and Piech - 2023 - The Student Zipf Theory Inferring Latent Structur.pdf:application/pdf},
}

@inproceedings{wang_deep_2017,
	address = {New York, NY, USA},
	series = {L@{S} '17},
	title = {Deep {Knowledge} {Tracing} {On} {Programming} {Exercises}},
	isbn = {978-1-4503-4450-0},
	url = {https://doi.org/10.1145/3051457.3053985},
	doi = {10.1145/3051457.3053985},
	abstract = {Modeling a student's knowledge state while she is solving exercises is a crucial stepping stone towards providing better personalized learning experiences at scale. This task, also referred to as "knowledge tracing", has been explored extensively on exercises where student submissions fall into a finite discrete solution space, e.g. a multiple-choice answer. However, we believe that rich information about a student's learning is captured within their responses to open-ended problems with unbounded solution spaces, such as programming exercises. In addition, sequential snapshots of a student's progress while she is solving a single exercise can provide valuable insights into her learning behavior. In this setting, creating representations for a student's knowledge state is a challenging task, but with recent advances in machine learning, there are more promising techniques to learn representations for complex entities. In our work, we feed the embedded program submissions into a recurrent neural network and train it on the task of predicting the student's success on the subsequent programming exercise. By training on this task, the model learns nuanced representations of a student's knowledge, and reliably predicts future student performance.},
	urldate = {2023-12-16},
	booktitle = {Proceedings of the {Fourth} (2017) {ACM} {Conference} on {Learning} @ {Scale}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Lisa and Sy, Angela and Liu, Larry and Piech, Chris},
	month = apr,
	year = {2017},
	keywords = {deep learning, educational data mining, knowledge tracing, machine learning, online education, personalized learning, representation learning, sequential modeling.},
	pages = {201--204},
}
