---
layout: post
title:  "AI Safety: is there an existential risk?"
tags: llm xrisk safety short
excerpt: "A few notes on the existential risk posed by artifical intelligence."
---

A few weeks ago, I had a conversation with [Jeffrey Ladish](https://jeffreyladish.com/) and [Eli Tyre](https://elityre.com/), two AI safety researchers and activists who put out a [call](https://twitter.com/JeffLadish/status/1848885766849761353) for open conversations with skepticism about "AI risk" arguments:

>I'm looking for people to have 30-60 minute conversations with about AI. I'm especially interested in talking with people who have heard some of the AI risk arguments, and have used ChatGPT, but don't see why or how AI could actually take over or seize control from humanity
>
>My plan is to briefly walk through the reasons why I am concerned about near-term existential AI risk, with space for you to ask questions or state objections, and then mostly have an informal conversation about whatever seems most relevant and interesting to the two of us

This discussion was interesting, because I've had pretty minimal exposure to AI "existential risk" arguments.
In general, I remain skeptical of existential risk arguments, but I'm interested in continuing to learn more.

A few of my personal take-aways from the conversation and from reflecting on the conversation afterward:
 - Existential risk arguments rely fundamentaly on logical or philosophical arguments. There are relatively few empirical questions related to existential AI risk that can be answered in principle, and those that can are probably only easy to answer in hindsight.
   - In general, I've done almost no thinking through of the philosophical issues. For that reason, social consensus among the people I perceive to be experts (e.g. computer science researchers, philosophers of AI, etc.) is important to me.
   - For example, is it possible to build a silicon mind? I have no idea. As someone with very little exposure to ideas like this, I find it essentially impossible to imagine "consciousness, but not human-like". But existential risk arguments seem to rely on non-human-like consciousness.
   - Generally, I agreed with Jeff and Eli about their empirical description of the current world.
 - Existential risk from AI is fundamentally about making predictions about the future, and I believe that humans are very bad forecasters.
 - It seems like getting a good signal for self-improvement is hard.
   - A system with a reliable self-improvement signal and sufficient resources could probably improve itself.
 - It seems like there's something distinct about the signal you get from "real life", as opposed to doing a lot of computing on data you already have, but it's hard to say what that is.
    - A Zoom call is a real reward signal! An AI system could certainly improve itself by talking with humans via Zoom, in the same way that humans improve ourselves by talking with humans via Zoom. But, it's slow; intuitively, you're constrained by communication speed. Do these constraints apply 

Resources recommended by Jeff and Eli at the conclusion of the call:
 - Robert Miles' [YouTube channel](https://www.youtube.com/@RobertMilesAI/featured)
 - <https://aisafety.info/>
 - <https://www.aisafety.com/>

Other resources I've stumbled across:
 - "The Compendium" by Connor Leahy, Gabriel Alfour, Chris Scammell, Andrea Miotti, Adam Shimi: <https://www.thecompendium.ai/>
   - "The Compendium aims to present a coherent worldview explaining the race to AGI and extinction risks and what to do about them, in a way that is accessible to non-technical readers who have no prior knowledge about AI."
   - I haven't really looked at this yet, so I'm not sure how useful it is.
 - "Debunking Robot Rights Metaphysically, Ethically, and Legally" by Abeba Birhane, Jelle van Dijk, Frank Pasquale: <https://arxiv.org/abs/2404.10072>
   - A useful overview of arguments for not accepting "robot rights" arguments, although the V1 draft appears pretty rough.
 - "Reclaiming AI as a Theoretical Tool for Cognitive Science" by Iris van Rooij, Olivia Guest, Federico Adolfi, Ronald de Haan, Antonina Kolokolova, Patricia Rich: <https://link.springer.com/article/10.1007/s42113-024-00217-5>
   - I've previously written about van Rooij's term [_makeism_]({% post_url 2023-10-17-wishful-mnemonics %}).
 - Consciousness
   - "The Hard Problem of Consciousness": <https://iep.utm.edu/hard-problem-of-conciousness/>
   - "Consciousness": <https://plato.stanford.edu/entries/consciousness/>

